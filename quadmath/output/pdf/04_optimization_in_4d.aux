\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Optimization in 4D}{1}{section.1}\protected@file@percent }
\newlabel{optimization-in-4d}{{1}{1}{Optimization in 4D}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Overview}{1}{subsection.1.1}\protected@file@percent }
\newlabel{overview}{{1.1}{1}{Overview}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Nelder--Mead on Integer Lattice}{1}{subsection.1.2}\protected@file@percent }
\newlabel{neldermead-on-integer-lattice}{{1.2}{1}{Nelder--Mead on Integer Lattice}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Parameters}{1}{subsubsection.1.2.1}\protected@file@percent }
\newlabel{parameters}{{1.2.1}{1}{Parameters}{subsubsection.1.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Volume-Level Dynamics}{2}{subsection.1.3}\protected@file@percent }
\newlabel{volume-level-dynamics}{{1.3}{2}{Volume-Level Dynamics}{subsection.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Pseudocode (Sketch)}{2}{subsection.1.4}\protected@file@percent }
\newlabel{pseudocode-sketch}{{1.4}{2}{Pseudocode (Sketch)}{subsection.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Figures}{2}{subsubsection.1.4.1}\protected@file@percent }
\newlabel{figures}{{1.4.1}{2}{Figures}{subsubsection.1.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Discrete Lattice Descent (Information-Theoretic Variant)}{2}{subsection.1.5}\protected@file@percent }
\newlabel{discrete-lattice-descent-information-theoretic-variant}{{1.5}{2}{Discrete Lattice Descent (Information-Theoretic Variant)}{subsection.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Convergence and Robustness}{2}{subsection.1.6}\protected@file@percent }
\newlabel{convergence-and-robustness}{{1.6}{2}{Convergence and Robustness}{subsection.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Discrete Nelder--Mead optimization trajectory on the integer Quadray lattice}. This time-series plot tracks key diagnostic quantities across 12 optimization iterations for a simple quadratic objective function defined on the integer Quadray lattice. \textbf  {X-axis}: Optimization iteration (0 through 12). \textbf  {Y-axis}: Key diagnostic values including objective function value (blue line), simplex volume (orange line), and maximum vertex spread (green line). \textbf  {Key observations}: The objective function decreases monotonically from iteration 0 to 12, showing convergence. The simplex volume (orange) exhibits discrete plateaus characteristic of integer-lattice optimization, where the Nelder--Mead algorithm can only move to integer coordinate positions. The maximum vertex spread (green) decreases as the simplex contracts around the optimum, indicating that the four vertices of the optimization tetrahedron are converging to a tight cluster. \textbf  {Discrete lattice behavior}: Unlike continuous optimization where the simplex can shrink to arbitrary precision, the integer Quadray lattice constrains the simplex to discrete volume levels, creating the characteristic step-like volume profile. This discrete behavior is captured in the MP4 animation (\lstinline !simplex\_animation.mp4!) and the diagnostic traces in the following figure. The final simplex volume is minimal on the integer lattice, representing a stable ``energy level'' where further discrete moves do not improve the objective function.}}{3}{figure.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Tetrahedron volume scaling relationships: Euclidean vs IVM unit conventions}. This plot demonstrates the mathematical relationship between edge length scaling and tetravolume under both Euclidean (XYZ) and IVM (synergetics) unit conventions. \textbf  {X-axis}: Edge length scaling factor (0.5 to 2.0). \textbf  {Y-axis}: Tetrahedron volume in respective units. \textbf  {Blue line (Euclidean)}: Volume scales as the cube of edge length, following the standard \(V = \frac  {\sqrt  {2}}{12} \cdot L^3\) relationship for regular tetrahedra. \textbf  {Orange line (IVM)}: Volume scales as the cube of edge length but in IVM tetra-units, following \(V_{ivm} = \frac  {1}{8} \cdot L^3\) where the regular tetrahedron with unit edge has volume 1/8. \textbf  {Key insight}: The ratio between these two scaling laws is the synergetics factor \(S3 = \sqrt  {9/8} \approx 1.06066\), which converts between Euclidean and IVM volume conventions. \textbf  {Discrete optimization context}: When working on the integer Quadray lattice, this scaling relationship helps diagnose whether volume changes are due to geometric scaling or discrete lattice effects. The plot shows that both conventions preserve the cubic scaling relationship, but with different fundamental units reflecting the different geometric assumptions of Coxeter.4D (Euclidean) versus Fuller.4D (synergetics) frameworks.}}{4}{figure.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Final converged simplex configuration in 3D embedding space}. This 3D scatter plot shows the four vertices of the Nelder--Mead simplex after 12 iterations of discrete optimization on the integer Quadray lattice. \textbf  {Points}: Four colored spheres representing the final simplex vertices, each positioned at integer Quadray coordinates projected to 3D space via the default embedding matrix. \textbf  {Colors}: Each vertex has a distinct color (blue, orange, green, red) for easy identification. \textbf  {Optimization context}: These vertices represent the final state of the discrete Nelder--Mead algorithm after converging to a local optimum on the integer lattice. The tight clustering of vertices indicates successful convergence, with the simplex having contracted around the optimal point. \textbf  {Lattice constraints}: All vertex positions correspond to integer Quadray coordinates, demonstrating the discrete nature of the optimization. The final simplex volume is minimal on the integer lattice, representing a stable configuration where further discrete moves do not improve the objective function. This visualization complements the time-series animation (\lstinline !simplex\_animation.mp4!) and the diagnostic traces in the previous figure. The final simplex volume is minimal on the integer lattice, representing a stable ``energy level'' where further discrete moves do not improve the objective function.}}{5}{figure.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Information-Geometric View (Einstein.4D analogy in metric form)}{6}{subsection.1.7}\protected@file@percent }
\newlabel{information-geometric-view-einstein.4d-analogy-in-metric-form}{{1.7}{6}{Information-Geometric View (Einstein.4D analogy in metric form)}{subsection.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Empirical Fisher Information Matrix (FIM) for noisy linear regression}. This heatmap visualizes the 3×3 Fisher information matrix \(F_{ij}\) estimated from per-sample gradients of a misspecified linear regression model. \textbf  {Matrix structure}: The FIM captures the local curvature of the log-likelihood surface around the current parameter estimate, with brighter colors indicating higher information content. \textbf  {Diagonal dominance}: The diagonal elements (F₀₀, F₁₁, F₂₂) show the strongest information content, indicating that each parameter contributes independently to the model's predictive power. \textbf  {Off-diagonal structure}: The off-diagonal elements reveal parameter interactions and potential redundancy in the model specification. \textbf  {Optimization implications}: This FIM structure guides natural gradient descent by weighting parameter updates according to local curvature, leading to more efficient convergence than standard gradient descent. The matrix is computed empirically from training data, making it adaptive to the specific data distribution and current parameter values. This empirical approach is particularly valuable when the true data-generating process is unknown or when working with complex, non-linear models where analytical FIM computation is intractable.}}{6}{figure.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Multi-Objective and Higher-Dimensional Notes (Coxeter.4D perspective)}{6}{subsection.1.8}\protected@file@percent }
\newlabel{multi-objective-and-higher-dimensional-notes-coxeter.4d-perspective}{{1.8}{6}{Multi-Objective and Higher-Dimensional Notes (Coxeter.4D perspective)}{subsection.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Fisher Information Matrix eigenspectrum: principal curvature directions}. This bar chart displays the eigenvalue decomposition of the empirical Fisher information matrix from the previous figure, revealing the principal curvature directions of the parameter manifold. \textbf  {X-axis}: Eigenvalue indices (0, 1, 2) sorted in descending order of magnitude. \textbf  {Y-axis}: Eigenvalue magnitudes representing the strength of curvature along each principal direction. \textbf  {Eigenvalue interpretation}: Larger eigenvalues indicate directions of high curvature (tight constraints) where the objective function changes rapidly with parameter changes. Smaller eigenvalues indicate directions of low curvature (loose constraints) where the objective function is relatively flat. \textbf  {Optimization geometry}: This eigenspectrum reveals the anisotropic nature of the parameter space, explaining why natural gradient descent (which scales updates by the inverse FIM) converges more efficiently than standard gradient descent. The principal directions provide insight into which parameter combinations are most sensitive to data changes and which are relatively stable. This geometric understanding is crucial for designing effective optimization strategies and understanding model behavior.}}{7}{figure.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Natural gradient descent trajectory on a quadratic objective (2D projection)}. This line plot with markers shows the parameter trajectory of natural gradient descent converging to the optimum of a quadratic objective function. \textbf  {Trajectory}: The blue line with markers traces the parameter evolution from initial guess to final optimum, showing the path taken through the 2D parameter space. \textbf  {Markers}: Each marker represents one optimization step, with spacing indicating the step size and convergence rate. \textbf  {Convergence behavior}: The trajectory shows smooth, direct convergence to the optimum, characteristic of natural gradient descent on well-conditioned objectives. \textbf  {Comparison with standard gradient descent}: Natural gradient descent typically produces more direct trajectories than standard gradient descent, especially on ill-conditioned problems where the parameter space has strong anisotropy. This efficiency comes from the FIM-based scaling that adapts step sizes to local curvature. The trajectory demonstrates how information-geometric optimization leverages the intrinsic geometry of the parameter space to achieve faster, more stable convergence than naive gradient methods.}}{8}{figure.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Variational free energy landscape for a discrete 2-state system}. This curve shows the variational free energy \(\symcal {F} = -\qopname  \relax o{log}P(o|s) + \text  {KL}[Q(s)||P(s)]\) (see Eq. \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:supp_free_energy}\unskip \@@italiccorr )}}) as a function of the variational distribution parameter. \textbf  {X-axis}: Variational parameter controlling the distribution over the two discrete states. \textbf  {Y-axis}: Free energy value in natural units. \textbf  {Curve shape}: The free energy exhibits a clear minimum at the optimal variational distribution, representing the best approximation to the true posterior given the constraints of the variational family. \textbf  {KL divergence component}: The free energy balances data fit (first term) with regularization (KL divergence from prior), preventing overfitting while maintaining good predictive performance. \textbf  {Optimization interpretation}: Minimizing this free energy corresponds to finding the best variational approximation to the true posterior, a fundamental task in Bayesian inference and active inference. The smooth, convex shape of the free energy landscape makes optimization straightforward using standard methods like gradient descent or natural gradient descent. This variational framework provides a principled approach to approximate inference in complex models where exact posterior computation is intractable.}}{9}{figure.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}External validation and computational context}{10}{subsection.1.9}\protected@file@percent }
\newlabel{external-validation-and-computational-context}{{1.9}{10}{External validation and computational context}{subsection.1.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10}Results}{10}{subsection.1.10}\protected@file@percent }
\newlabel{results}{{1.10}{10}{Results}{subsection.1.10}{}}
\gdef \@abspage@last{10}
